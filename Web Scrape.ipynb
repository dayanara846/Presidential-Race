{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83dc6244",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "The purpose of this code is to web scrape data from New York Times articles that covered each winning candidate's presidential election since 1960. Since most presidential candidates start their campaigns more than six months before elections, I decided to analyze data since January of the election year up until the last day before election day (November 3rd). All US presidents were famous before winning the elections, therefore they started in many New York Times' articles well before winning the presidential race. Therefore, to filter for unrelated articles, I included the keywords \"presidential candidate\" preceded by the name of each candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e9861",
   "metadata": {},
   "source": [
    "# Basic variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa6fb153",
   "metadata": {},
   "outputs": [],
   "source": [
    "Presidents = [\"Joe%20R.%20Biden\", \"Donald%20J.%20Trump\", \"Barack%20H.%20Obama\", \"Barack%20H.%20Obama\", \"George%20W.%20Bush\", \"George%20W.%20Bush\", \"Bill%20Clinton\", \"Ronald%20Reagan\", \"Ronald%20Reagan\", \"Jimmy%20Carter\", \"Richard%20Nixon\", \"Richard%20Nixon\", \"Lyndon%20B.%20Johnson\", \"John%20F.%20Kennedy\"]\n",
    "Months_end = [3,6,9,12]\n",
    "Months_start = [1, 4, 7, 10]\n",
    "End_days = [31, 30, 30, 31]\n",
    "\n",
    "Years = list(range(1960,2021)).sort(reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d40774",
   "metadata": {},
   "source": [
    "Joe R. Biden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d081c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a0e43c58c77f>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x005C6903+2517251]\n",
      "\tOrdinal0 [0x0055F8E1+2095329]\n",
      "\tOrdinal0 [0x00462848+1058888]\n",
      "\tOrdinal0 [0x0048D448+1233992]\n",
      "\tOrdinal0 [0x0048D63B+1234491]\n",
      "\tOrdinal0 [0x004B7812+1406994]\n",
      "\tOrdinal0 [0x004A650A+1336586]\n",
      "\tOrdinal0 [0x004B5BBF+1399743]\n",
      "\tOrdinal0 [0x004A639B+1336219]\n",
      "\tOrdinal0 [0x004827A7+1189799]\n",
      "\tOrdinal0 [0x00483609+1193481]\n",
      "\tGetHandleVerifier [0x00755904+1577972]\n",
      "\tGetHandleVerifier [0x00800B97+2279047]\n",
      "\tGetHandleVerifier [0x00656D09+534521]\n",
      "\tGetHandleVerifier [0x00655DB9+530601]\n",
      "\tOrdinal0 [0x00564FF9+2117625]\n",
      "\tOrdinal0 [0x005698A8+2136232]\n",
      "\tOrdinal0 [0x005699E2+2136546]\n",
      "\tOrdinal0 [0x00573541+2176321]\n",
      "\tBaseThreadInitThunk [0x758BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x76F07A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x76F07A6E+238]\n",
      "\n",
      "Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a0e43c58c77f>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x005C6903+2517251]\n",
      "\tOrdinal0 [0x0055F8E1+2095329]\n",
      "\tOrdinal0 [0x00462848+1058888]\n",
      "\tOrdinal0 [0x0048D448+1233992]\n",
      "\tOrdinal0 [0x0048D63B+1234491]\n",
      "\tOrdinal0 [0x004B7812+1406994]\n",
      "\tOrdinal0 [0x004A650A+1336586]\n",
      "\tOrdinal0 [0x004B5BBF+1399743]\n",
      "\tOrdinal0 [0x004A639B+1336219]\n",
      "\tOrdinal0 [0x004827A7+1189799]\n",
      "\tOrdinal0 [0x00483609+1193481]\n",
      "\tGetHandleVerifier [0x00755904+1577972]\n",
      "\tGetHandleVerifier [0x00800B97+2279047]\n",
      "\tGetHandleVerifier [0x00656D09+534521]\n",
      "\tGetHandleVerifier [0x00655DB9+530601]\n",
      "\tOrdinal0 [0x00564FF9+2117625]\n",
      "\tOrdinal0 [0x005698A8+2136232]\n",
      "\tOrdinal0 [0x005699E2+2136546]\n",
      "\tOrdinal0 [0x00573541+2176321]\n",
      "\tBaseThreadInitThunk [0x758BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x76F07A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x76F07A6E+238]\n",
      "\n",
      "Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a0e43c58c77f>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x005C6903+2517251]\n",
      "\tOrdinal0 [0x0055F8E1+2095329]\n",
      "\tOrdinal0 [0x00462848+1058888]\n",
      "\tOrdinal0 [0x0048D448+1233992]\n",
      "\tOrdinal0 [0x0048D63B+1234491]\n",
      "\tOrdinal0 [0x004B7812+1406994]\n",
      "\tOrdinal0 [0x004A650A+1336586]\n",
      "\tOrdinal0 [0x004B5BBF+1399743]\n",
      "\tOrdinal0 [0x004A639B+1336219]\n",
      "\tOrdinal0 [0x004827A7+1189799]\n",
      "\tOrdinal0 [0x00483609+1193481]\n",
      "\tGetHandleVerifier [0x00755904+1577972]\n",
      "\tGetHandleVerifier [0x00800B97+2279047]\n",
      "\tGetHandleVerifier [0x00656D09+534521]\n",
      "\tGetHandleVerifier [0x00655DB9+530601]\n",
      "\tOrdinal0 [0x00564FF9+2117625]\n",
      "\tOrdinal0 [0x005698A8+2136232]\n",
      "\tOrdinal0 [0x005699E2+2136546]\n",
      "\tOrdinal0 [0x00573541+2176321]\n",
      "\tBaseThreadInitThunk [0x758BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x76F07A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x76F07A6E+238]\n",
      "\n",
      "Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a0e43c58c77f>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x005C6903+2517251]\n",
      "\tOrdinal0 [0x0055F8E1+2095329]\n",
      "\tOrdinal0 [0x00462848+1058888]\n",
      "\tOrdinal0 [0x0048D448+1233992]\n",
      "\tOrdinal0 [0x0048D63B+1234491]\n",
      "\tOrdinal0 [0x004B7812+1406994]\n",
      "\tOrdinal0 [0x004A650A+1336586]\n",
      "\tOrdinal0 [0x004B5BBF+1399743]\n",
      "\tOrdinal0 [0x004A639B+1336219]\n",
      "\tOrdinal0 [0x004827A7+1189799]\n",
      "\tOrdinal0 [0x00483609+1193481]\n",
      "\tGetHandleVerifier [0x00755904+1577972]\n",
      "\tGetHandleVerifier [0x00800B97+2279047]\n",
      "\tGetHandleVerifier [0x00656D09+534521]\n",
      "\tGetHandleVerifier [0x00655DB9+530601]\n",
      "\tOrdinal0 [0x00564FF9+2117625]\n",
      "\tOrdinal0 [0x005698A8+2136232]\n",
      "\tOrdinal0 [0x005699E2+2136546]\n",
      "\tOrdinal0 [0x00573541+2176321]\n",
      "\tBaseThreadInitThunk [0x758BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x76F07A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x76F07A6E+238]\n",
      "\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2020\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20200331&query=Joe%20R.%20Biden&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20200101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20200630&query=Joe%20R.%20Biden&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20200401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20200930&query=Joe%20R.%20Biden&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20200701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20201231&query=Joe%20R.%20Biden&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20201001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2020_Joe_Biden.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a82cf",
   "metadata": {},
   "source": [
    "# Election 2016 - Donald J. Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c20aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:23: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:80: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:130: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:180: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:229: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:280: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:329: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: element click intercepted: Element <button data-testid=\"search-show-more-button\" type=\"button\">...</button> is not clickable at point (509, 633). Other element would receive the click: <div class=\"css-1t62hi8\">...</div>\n",
      "  (Session info: chrome=96.0.4664.45)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x00263509+1258761]\n",
      "\tOrdinal0 [0x00261868+1251432]\n",
      "\tOrdinal0 [0x0025F65D+1242717]\n",
      "\tOrdinal0 [0x0025E4A8+1238184]\n",
      "\tOrdinal0 [0x00254037+1196087]\n",
      "\tOrdinal0 [0x002764D3+1336531]\n",
      "\tOrdinal0 [0x00253A36+1194550]\n",
      "\tOrdinal0 [0x002765BA+1336762]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:383: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:469: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:520: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: element click intercepted: Element <button data-testid=\"search-show-more-button\" type=\"button\">...</button> is not clickable at point (509, 636). Other element would receive the click: <div class=\"css-1t62hi8\">...</div>\n",
      "  (Session info: chrome=96.0.4664.45)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x00263509+1258761]\n",
      "\tOrdinal0 [0x00261868+1251432]\n",
      "\tOrdinal0 [0x0025F65D+1242717]\n",
      "\tOrdinal0 [0x0025E4A8+1238184]\n",
      "\tOrdinal0 [0x00254037+1196087]\n",
      "\tOrdinal0 [0x002764D3+1336531]\n",
      "\tOrdinal0 [0x00253A36+1194550]\n",
      "\tOrdinal0 [0x002765BA+1336762]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-8b0322ca85c0>:572: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: chrome not reachable\n",
      "  (Session info: chrome=96.0.4664.45)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232710+1058576]\n",
      "\tOrdinal0 [0x002276A4+1013412]\n",
      "\tOrdinal0 [0x00227EA8+1015464]\n",
      "\tOrdinal0 [0x00229695+1021589]\n",
      "\tOrdinal0 [0x00223686+996998]\n",
      "\tOrdinal0 [0x00233A60+1063520]\n",
      "\tOrdinal0 [0x00285382+1397634]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=96.0.4664.45)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00396903+2517251]\n\tOrdinal0 [0x0032F8E1+2095329]\n\tOrdinal0 [0x00232710+1058576]\n\tOrdinal0 [0x002276A4+1013412]\n\tOrdinal0 [0x00227EA8+1015464]\n\tOrdinal0 [0x00229695+1021589]\n\tOrdinal0 [0x00223686+996998]\n\tOrdinal0 [0x00233A60+1063520]\n\tOrdinal0 [0x00285382+1397634]\n\tOrdinal0 [0x0027639B+1336219]\n\tOrdinal0 [0x002527A7+1189799]\n\tOrdinal0 [0x00253609+1193481]\n\tGetHandleVerifier [0x00525904+1577972]\n\tGetHandleVerifier [0x005D0B97+2279047]\n\tGetHandleVerifier [0x00426D09+534521]\n\tGetHandleVerifier [0x00425DB9+530601]\n\tOrdinal0 [0x00334FF9+2117625]\n\tOrdinal0 [0x003398A8+2136232]\n\tOrdinal0 [0x003399E2+2136546]\n\tOrdinal0 [0x00343541+2176321]\n\tBaseThreadInitThunk [0x7736FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8b0322ca85c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    584\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[0msearch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ol'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'data-testid'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'search-results'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mpage_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    924\u001b[0m                 \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m         \"\"\"\n\u001b[1;32m--> 926\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET_PAGE_SOURCE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    426\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=96.0.4664.45)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00396903+2517251]\n\tOrdinal0 [0x0032F8E1+2095329]\n\tOrdinal0 [0x00232710+1058576]\n\tOrdinal0 [0x002276A4+1013412]\n\tOrdinal0 [0x00227EA8+1015464]\n\tOrdinal0 [0x00229695+1021589]\n\tOrdinal0 [0x00223686+996998]\n\tOrdinal0 [0x00233A60+1063520]\n\tOrdinal0 [0x00285382+1397634]\n\tOrdinal0 [0x0027639B+1336219]\n\tOrdinal0 [0x002527A7+1189799]\n\tOrdinal0 [0x00253609+1193481]\n\tGetHandleVerifier [0x00525904+1577972]\n\tGetHandleVerifier [0x005D0B97+2279047]\n\tGetHandleVerifier [0x00426D09+534521]\n\tGetHandleVerifier [0x00425DB9+530601]\n\tOrdinal0 [0x00334FF9+2117625]\n\tOrdinal0 [0x003398A8+2136232]\n\tOrdinal0 [0x003399E2+2136546]\n\tOrdinal0 [0x00343541+2176321]\n\tBaseThreadInitThunk [0x7736FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2019\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20190228&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20190101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_0 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20190331&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20190301&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20190530&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20190401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20190630&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20190601&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20190730&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20190701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20190930&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20190801&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20191130&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20191001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20191231&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20191201&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_5 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_0, my_dat_1, my_dat_2_1, my_dat_2, my_dat_3, my_dat_4, my_dat_5], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2019_Donald_Trump.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2018\n",
    "\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20180228&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20180101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_0 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20180331&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20180301&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20180530&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20180401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20180630&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20180601&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20180730&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20180701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20180930&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20180801&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20181130&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20181001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20181231&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20181201&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_5 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_0, my_dat_1, my_dat_2_1, my_dat_2, my_dat_3, my_dat_4, my_dat_5], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2018_Donald_Trump.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2017\n",
    "\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20170228&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20170101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_0 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20170331&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20170301&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20170530&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20170401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20170630&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20170601&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20170730&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20170701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20170930&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20170801&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20171130&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20171001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20171231&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20171201&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_5 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_0, my_dat_1, my_dat_2_1, my_dat_2, my_dat_3, my_dat_4, my_dat_5], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2017_Donald_Trump.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2016\n",
    "\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20160228&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20160101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_0 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20160331&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20160301&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20160530&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20160401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20160630&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20160601&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20160730&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20160701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20160930&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20160801&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20161130&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20161001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20161231&query=Donald%20J.%20Trump&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20161201&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_5 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_0, my_dat_1, my_dat_2_1, my_dat_2, my_dat_3, my_dat_4, my_dat_5], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2016_Donald_Trump.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6e01c",
   "metadata": {},
   "source": [
    "# Election 2012 - Barack H. Obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b1f43f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:260: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:311: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:363: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:412: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:495: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:546: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:598: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:647: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:730: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:781: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:833: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-dd8c86664734>:882: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################---------------------------------- 2015\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20150331&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20150101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20150630&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20150401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20150930&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20150701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20151231&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20151001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2015_Barack_Obama.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2014\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20140331&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20140101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20140630&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20140401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20140930&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20140701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20141231&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20141001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2014_Barack_Obama.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2013\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20130331&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20130101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20130630&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20130401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20130930&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20130701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20131231&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20131001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2013_Barack_Obama.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2012\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20120331&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20120101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20120630&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20120401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20120930&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20120701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20121231&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20121001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2012_Barack_Obama.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581f381",
   "metadata": {},
   "source": [
    "# Election 2008 - Barack H. Obama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83a47e8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:260: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:311: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:363: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:412: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:495: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:546: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:598: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:647: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:730: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:781: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:833: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0255d4ddf36>:882: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################---------------------------------- 2011\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20110331&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20110101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20110630&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20110401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20110930&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20110701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20111231&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20111001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2011_Barack_Obama.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2010\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20100331&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20100101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20100630&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20100401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20100930&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20100701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20101231&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20101001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2010_Barack_Obama.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2009\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20090331&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20090101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20090630&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20090401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20090930&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20090701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20091231&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20091001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2009_Barack_Obama.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2008\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20080331&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20080101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20080630&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20080401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20080930&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20080701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20081231&query=Barack%20H.%20Obama&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20081001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2008_Barack_Obama.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1c6a2",
   "metadata": {},
   "source": [
    "# Election 2004 - George W. Bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac11bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:260: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:311: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:363: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:412: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:495: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:546: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:598: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:647: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:730: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:781: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:833: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-01d78a0e3416>:882: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00B26903+2517251]\n",
      "\tOrdinal0 [0x00ABF8E1+2095329]\n",
      "\tOrdinal0 [0x009C2848+1058888]\n",
      "\tOrdinal0 [0x009ED448+1233992]\n",
      "\tOrdinal0 [0x009ED63B+1234491]\n",
      "\tOrdinal0 [0x00A17812+1406994]\n",
      "\tOrdinal0 [0x00A0650A+1336586]\n",
      "\tOrdinal0 [0x00A15BBF+1399743]\n",
      "\tOrdinal0 [0x00A0639B+1336219]\n",
      "\tOrdinal0 [0x009E27A7+1189799]\n",
      "\tOrdinal0 [0x009E3609+1193481]\n",
      "\tGetHandleVerifier [0x00CB5904+1577972]\n",
      "\tGetHandleVerifier [0x00D60B97+2279047]\n",
      "\tGetHandleVerifier [0x00BB6D09+534521]\n",
      "\tGetHandleVerifier [0x00BB5DB9+530601]\n",
      "\tOrdinal0 [0x00AC4FF9+2117625]\n",
      "\tOrdinal0 [0x00AC98A8+2136232]\n",
      "\tOrdinal0 [0x00AC99E2+2136546]\n",
      "\tOrdinal0 [0x00AD3541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################---------------------------------- 2007\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20070331&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20070101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20070630&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20070401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20070930&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20070701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20071231&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20071001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2007_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2006\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20060331&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20060101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20060630&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20060401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20060930&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20060701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20061231&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20061001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2006_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2005\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20050331&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20060101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20050630&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20050401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20050930&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20050701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20051231&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20051001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2005_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2004\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20040331&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20040101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20040630&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20040401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20040930&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20040701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20041231&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20041001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2004_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b538297",
   "metadata": {},
   "source": [
    "# Election 2000 - George W. Bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13730528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-0544a70552f9>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0544a70552f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mshow_more\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_to_be_clickable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'//button[@type=\"button\"][contains(.,\"Show More\")]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mshow_more\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################---------------------------------- 2003\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20030331&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20030101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20030630&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20030401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20030930&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20030701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20031231&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20031001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2003_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2002\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20020331&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20020101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20020630&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20020401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20020930&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20020701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20021231&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20021001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2002_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2001\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20010331&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20010101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20010630&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20010401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20010930&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20010701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20011231&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20011001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2001_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 2000\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20000331&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20000101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20000630&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20000401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20000930&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20000701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20001031&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20001001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20001130&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20001101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20001231&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20001201&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_5 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4, my_dat_5], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2000_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b51b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-2c5d38b9f43b>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-2c5d38b9f43b>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-2c5d38b9f43b>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-2c5d38b9f43b>:175: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-2c5d38b9f43b>:227: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-2c5d38b9f43b>:284: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################---------------------------------- 2000\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20000331&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20000101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20000630&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20000401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20000930&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20000701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20001031&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20001001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20001130&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20001101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=20001231&query=George%20W.%20Bush&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=20001201&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_5 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4_1, my_dat_4, my_dat_5], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '2000_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4ca00",
   "metadata": {},
   "source": [
    "# Election 1996 - Bill Clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd05717f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:256: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:307: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:359: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:408: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:491: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:542: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:594: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:643: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:725: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:776: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:828: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f909203c852f>:877: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1999\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19990331&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19990101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19990630&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19990401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19990930&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19990701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19991231&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19991001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1999_Bill_Clinton.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1998\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19980331&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19980101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19980630&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19980401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19980930&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19980701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19981231&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19981001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1998_Bill_Clinton.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1997\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19970331&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19970101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19970630&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19970401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19970930&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19970701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19971231&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19971001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1997_Bill_Clinton.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1996\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19960331&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19960101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19960630&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19960401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19960930&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19960701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19961231&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19961001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1996_Bill_Clinton.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c868cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8da4bc59",
   "metadata": {},
   "source": [
    "# Election 1992 - Bill Clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "487f4f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:256: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:307: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:359: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:408: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:491: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:542: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:594: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:643: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7bdb3b2b9e10>:725: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00DC6903+2517251]\n",
      "\tOrdinal0 [0x00D5F8E1+2095329]\n",
      "\tOrdinal0 [0x00C62848+1058888]\n",
      "\tOrdinal0 [0x00C8D448+1233992]\n",
      "\tOrdinal0 [0x00C8D63B+1234491]\n",
      "\tOrdinal0 [0x00CB7812+1406994]\n",
      "\tOrdinal0 [0x00CA650A+1336586]\n",
      "\tOrdinal0 [0x00CB5BBF+1399743]\n",
      "\tOrdinal0 [0x00CA639B+1336219]\n",
      "\tOrdinal0 [0x00C827A7+1189799]\n",
      "\tOrdinal0 [0x00C83609+1193481]\n",
      "\tGetHandleVerifier [0x00F55904+1577972]\n",
      "\tGetHandleVerifier [0x01000B97+2279047]\n",
      "\tGetHandleVerifier [0x00E56D09+534521]\n",
      "\tGetHandleVerifier [0x00E55DB9+530601]\n",
      "\tOrdinal0 [0x00D64FF9+2117625]\n",
      "\tOrdinal0 [0x00D698A8+2136232]\n",
      "\tOrdinal0 [0x00D699E2+2136546]\n",
      "\tOrdinal0 [0x00D73541+2176321]\n",
      "\tBaseThreadInitThunk [0x767BFA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77D17A6E+238]\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7bdb3b2b9e10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[0msearch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ol'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'data-testid'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'search-results'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[0mlink_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1995\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19950331&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19950101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19950630&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19950401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19950930&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19950701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19951231&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19951001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1995_Bill_Clinton.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1994\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19940331&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19940101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19940630&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19940401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19940930&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19940701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19941231&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19941001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1994_Bill_Clinton.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1993\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19930331&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19930101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19930630&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19930401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19930930&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19930701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19931231&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19931001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1993_Bill_Clinton.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1992\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19920331&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19920101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19920630&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19920401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19920930&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19920701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19921231&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19921001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1992_Bill_Clinton.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1407827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8edf6dc024bd>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8edf6dc024bd>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8edf6dc024bd>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8edf6dc024bd>:172: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8edf6dc024bd>:223: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################---------------------------------- 1992\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19920331&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19920101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19920630&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19920401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19920930&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19920701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19921031&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19921001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19921231&query=Bill%20Clinton&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=newest&startDate=19921101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_5 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4, my_dat_5], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1992_Bill_Clinton.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b8ad1",
   "metadata": {},
   "source": [
    "# Election 1988 - George H. W. Bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad75889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:256: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:307: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:359: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:408: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:491: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:542: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:594: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:643: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:725: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:776: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:828: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2582610928e>:877: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1991\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19910331&query=George%20H.%20W.%20Bush&sort=newest&startDate=19910101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19910630&query=George%20H.%20W.%20Bush&sort=newest&startDate=19910401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19910930&query=George%20H.%20W.%20Bush&sort=newest&startDate=19910701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19911231&query=George%20H.%20W.%20Bush&sort=newest&startDate=19911001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1991_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1990\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19900331&query=George%20H.%20W.%20Bush&sort=newest&startDate=19900101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19900630&query=George%20H.%20W.%20Bush&sort=newest&startDate=19900401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19900930&query=George%20H.%20W.%20Bush&sort=newest&startDate=19900701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19901231&query=George%20H.%20W.%20Bush&sort=newest&startDate=19901001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1990_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1989\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19890331&query=George%20H.%20W.%20Bush&sort=newest&startDate=19890101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19890630&query=George%20H.%20W.%20Bush&sort=newest&startDate=19890401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19890930&query=George%20H.%20W.%20Bush&sort=newest&startDate=19890701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19891231&query=George%20H.%20W.%20Bush&sort=newest&startDate=19891001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1989_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1988\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19880331&query=George%20H.%20W.%20Bush&sort=newest&startDate=19880101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19880630&query=George%20H.%20W.%20Bush&sort=newest&startDate=19880401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19880930&query=George%20H.%20W.%20Bush&sort=newest&startDate=19880701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19881231&query=George%20H.%20W.%20Bush&sort=newest&startDate=19881001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1988_George_Bush.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d4f417",
   "metadata": {},
   "source": [
    "# Election 1984 - Ronald Reagan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9269d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-fa37768c6598>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-fa37768c6598>:85: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-fa37768c6598>:136: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-fa37768c6598>:188: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-fa37768c6598>:236: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-fa37768c6598>:289: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-fa37768c6598>:372: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-fa37768c6598>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mshow_more\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_to_be_clickable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'//button[@type=\"button\"][contains(.,\"Show More\")]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[0mshow_more\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py\u001b[0m in \u001b[0;36muntil\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py\u001b[0m in \u001b[0;36m_predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWebElement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if given locator instead of WebElement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# grab element at locator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisibility_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m   1242\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'[name=\"%s\"]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0m\u001b[0;32m   1245\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m             'value': value})['value']\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{self._url}{path}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     76\u001b[0m             )\n\u001b[0;32m     77\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             return self.request_encode_body(\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1987\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19870228&query=Ronald%20Reagan&sort=newest&startDate=19870101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_0 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19870331&query=Ronald%20Reagan&sort=newest&startDate=19870301&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19870630&query=Ronald%20Reagan&sort=newest&startDate=19870401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19870930&query=Ronald%20Reagan&sort=newest&startDate=19870701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19871130&query=Ronald%20Reagan&sort=newest&startDate=19871001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19871231&query=Ronald%20Reagan&sort=newest&startDate=19871201&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_5 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_0, my_dat_1, my_dat_2, my_dat_3, my_dat_4, my_dat_5], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1987_Ronald_Reagan.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1986\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19860331&query=Ronald%20Reagan&sort=newest&startDate=19860101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19860630&query=Ronald%20Reagan&sort=newest&startDate=19860401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19860930&query=Ronald%20Reagan&sort=newest&startDate=19860701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19861231&query=Ronald%20Reagan&sort=newest&startDate=19861001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1986_Ronald_Reagan.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1985\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19850331&query=Ronald%20Reagan&sort=newest&startDate=19850101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19850630&query=Ronald%20Reagan&sort=newest&startDate=19850401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19850930&query=Ronald%20Reagan&sort=newest&startDate=19850701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19851231&query=Ronald%20Reagan&sort=newest&startDate=19851001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1985_Ronald_Reagan.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1984\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19840331&query=Ronald%20Reagan&sort=newest&startDate=19840101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19840630&query=Ronald%20Reagan&sort=newest&startDate=19840401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19840930&query=Ronald%20Reagan&sort=newest&startDate=19840701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19841231&query=Ronald%20Reagan&sort=newest&startDate=19841001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1984_Ronald_Reagan.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2961c9",
   "metadata": {},
   "source": [
    "# Election 1980 - Ronald Reagan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d99b7beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4e82ab2d6382>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4e82ab2d6382>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4e82ab2d6382>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4e82ab2d6382>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4e82ab2d6382>:256: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4e82ab2d6382>:307: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4e82ab2d6382>:359: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4e82ab2d6382>:408: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-4e82ab2d6382>:491: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: chrome not reachable\n",
      "  (Session info: chrome=96.0.4664.45)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232710+1058576]\n",
      "\tOrdinal0 [0x002276A4+1013412]\n",
      "\tOrdinal0 [0x00227EA8+1015464]\n",
      "\tOrdinal0 [0x00229695+1021589]\n",
      "\tOrdinal0 [0x00223686+996998]\n",
      "\tOrdinal0 [0x00233A60+1063520]\n",
      "\tOrdinal0 [0x00285382+1397634]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=96.0.4664.45)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00396903+2517251]\n\tOrdinal0 [0x0032F8E1+2095329]\n\tOrdinal0 [0x00232710+1058576]\n\tOrdinal0 [0x002276A4+1013412]\n\tOrdinal0 [0x00227EA8+1015464]\n\tOrdinal0 [0x00229695+1021589]\n\tOrdinal0 [0x00223686+996998]\n\tOrdinal0 [0x00233A60+1063520]\n\tOrdinal0 [0x00285382+1397634]\n\tOrdinal0 [0x0027639B+1336219]\n\tOrdinal0 [0x002527A7+1189799]\n\tOrdinal0 [0x00253609+1193481]\n\tGetHandleVerifier [0x00525904+1577972]\n\tGetHandleVerifier [0x005D0B97+2279047]\n\tGetHandleVerifier [0x00426D09+534521]\n\tGetHandleVerifier [0x00425DB9+530601]\n\tOrdinal0 [0x00334FF9+2117625]\n\tOrdinal0 [0x003398A8+2136232]\n\tOrdinal0 [0x003399E2+2136546]\n\tOrdinal0 [0x00343541+2176321]\n\tBaseThreadInitThunk [0x7736FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4e82ab2d6382>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m \u001b[0msearch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ol'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'data-testid'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'search-results'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mpage_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    924\u001b[0m                 \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m         \"\"\"\n\u001b[1;32m--> 926\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET_PAGE_SOURCE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    426\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_KT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m_VT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=96.0.4664.45)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00396903+2517251]\n\tOrdinal0 [0x0032F8E1+2095329]\n\tOrdinal0 [0x00232710+1058576]\n\tOrdinal0 [0x002276A4+1013412]\n\tOrdinal0 [0x00227EA8+1015464]\n\tOrdinal0 [0x00229695+1021589]\n\tOrdinal0 [0x00223686+996998]\n\tOrdinal0 [0x00233A60+1063520]\n\tOrdinal0 [0x00285382+1397634]\n\tOrdinal0 [0x0027639B+1336219]\n\tOrdinal0 [0x002527A7+1189799]\n\tOrdinal0 [0x00253609+1193481]\n\tGetHandleVerifier [0x00525904+1577972]\n\tGetHandleVerifier [0x005D0B97+2279047]\n\tGetHandleVerifier [0x00426D09+534521]\n\tGetHandleVerifier [0x00425DB9+530601]\n\tOrdinal0 [0x00334FF9+2117625]\n\tOrdinal0 [0x003398A8+2136232]\n\tOrdinal0 [0x003399E2+2136546]\n\tOrdinal0 [0x00343541+2176321]\n\tBaseThreadInitThunk [0x7736FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1983\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19830331&query=Ronald%20Reagan&sort=newest&startDate=19830101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19830630&query=Ronald%20Reagan&sort=newest&startDate=19830401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19830930&query=Ronald%20Reagan&sort=newest&startDate=19830701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19831231&query=Ronald%20Reagan&sort=newest&startDate=19831001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1983_Ronald_Reagan.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1982\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19820331&query=Ronald%20Reagan&sort=newest&startDate=19820101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19820630&query=Ronald%20Reagan&sort=newest&startDate=19820401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19820930&query=Ronald%20Reagan&sort=newest&startDate=19820701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19821231&query=Ronald%20Reagan&sort=newest&startDate=19821001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1982_Ronald_Reagan.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1981\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19810331&query=Ronald%20Reagan&sort=newest&startDate=19810101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19810630&query=Ronald%20Reagan&sort=newest&startDate=19810401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19810930&query=Ronald%20Reagan&sort=newest&startDate=19810701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19811231&query=Ronald%20Reagan&sort=newest&startDate=19811001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1981_Ronald_Reagan.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1980\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19800331&query=Ronald%20Reagan&sort=newest&startDate=19800101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19800630&query=Ronald%20Reagan&sort=newest&startDate=19800401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19800930&query=Ronald%20Reagan&sort=newest&startDate=19800701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19801231&query=Ronald%20Reagan&sort=newest&startDate=19801001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1980_Ronald_Reagan.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c742f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-24103001f3fa>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-24103001f3fa>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-24103001f3fa>:123: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-24103001f3fa>:172: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-24103001f3fa>:221: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-24103001f3fa>:271: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-24103001f3fa>:320: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-24103001f3fa>:367: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1980\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19800331&query=Ronald%20Reagan&sort=newest&startDate=19800101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19800531&query=Ronald%20Reagan&sort=newest&startDate=19800401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19800630&query=Ronald%20Reagan&sort=newest&startDate=19800601&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19800831&query=Ronald%20Reagan&sort=newest&startDate=19800701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19800930&query=Ronald%20Reagan&sort=newest&startDate=19800901&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19801031&query=Ronald%20Reagan&sort=newest&startDate=19801001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19801130&query=Ronald%20Reagan&sort=newest&startDate=19801101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4_2 = my_dat\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19801231&query=Ronald%20Reagan&sort=newest&startDate=19801201&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_2_1, my_dat_3, my_dat_3_1, my_dat_4_1, my_dat_4_2, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1980_Ronald_Reagan.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce14c194",
   "metadata": {},
   "source": [
    "# Election 1976 - Jimmy Carter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38513d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:256: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:307: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:359: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:408: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:490: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:541: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:593: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:642: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:724: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-921b4b49c881>:775: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-921b4b49c881>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m         \u001b[0mshow_more\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_to_be_clickable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'//button[@type=\"button\"][contains(.,\"Show More\")]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    784\u001b[0m         \u001b[0mshow_more\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py\u001b[0m in \u001b[0;36muntil\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py\u001b[0m in \u001b[0;36m_predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWebElement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if given locator instead of WebElement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# grab element at locator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisibility_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py\u001b[0m in \u001b[0;36m_predicate\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_element_if_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_predicate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py\u001b[0m in \u001b[0;36m_element_if_visible\u001b[1;34m(element, visibility)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_element_if_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisibility\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_displayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvisibility\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mis_displayed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[1;34m\"\"\"Whether the element is visible to a user.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[1;31m# Only go into this conditional for browsers that don't use the atom themselves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 564\u001b[1;33m         return self.parent.execute_script(\n\u001b[0m\u001b[0;32m    565\u001b[0m             \u001b[1;34m\"return (%s).apply(null, arguments);\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0misDisplayed_js\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m             self)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    876\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW3C_EXECUTE_SCRIPT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m         return self.execute(command, {\n\u001b[0m\u001b[0;32m    879\u001b[0m             \u001b[1;34m'script'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m             'args': converted_args})['value']\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{self._url}{path}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     76\u001b[0m             )\n\u001b[0;32m     77\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             return self.request_encode_body(\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1979\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19790331&query=Jimmy%20Carter&sort=newest&startDate=19790101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19790630&query=Jimmy%20Carter&sort=newest&startDate=19790401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19790930&query=Jimmy%20Carter&sort=newest&startDate=19790701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19791231&query=Jimmy%20Carter&sort=newest&startDate=19791001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1979_Jimmy_Carter.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1978\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19780331&query=Jimmy%20Carter&sort=newest&startDate=19780101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19780630&query=Jimmy%20Carter&sort=newest&startDate=19780401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19780930&query=Jimmy%20Carter&sort=newest&startDate=19780701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19781231&query=Jimmy%20Carter&sort=newest&startDate=19781001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1978_Jimmy_Carter.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1977\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19770331&query=Jimmy%20Carter&sort=newest&startDate=19770101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19770630&query=Jimmy%20Carter&sort=newest&startDate=19770401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19770930&query=Jimmy%20Carter&sort=newest&startDate=19770701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19771231&query=Jimmy%20Carter&sort=newest&startDate=19771001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1977_Jimmy_Carter.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1976\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19760331&query=Jimmy%20Carter&sort=newest&startDate=19760101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_0 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19760531&query=Jimmy%20Carter&sort=newest&startDate=19760401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19760630&query=Jimmy%20Carter&sort=newest&startDate=19760601&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19760930&query=Jimmy%20Carter&sort=newest&startDate=19760701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19761231&query=Jimmy%20Carter&sort=newest&startDate=19761001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_0, my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1976_Jimmy_Carter.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fe1a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-55dcc85aacac>:23: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-55dcc85aacac>:75: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-55dcc85aacac>:126: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-55dcc85aacac>:176: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-55dcc85aacac>:227: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-55dcc85aacac>:276: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-55dcc85aacac>:326: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "########################---------------------------------- 1976\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19760331&query=Jimmy%20Carter&sort=newest&startDate=19760101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_0 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19760531&query=Jimmy%20Carter&sort=newest&startDate=19760401&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19760630&query=Jimmy%20Carter&sort=newest&startDate=19760601&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19760731&query=Jimmy%20Carter&sort=newest&startDate=19760701&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19760930&query=Jimmy%20Carter&sort=newest&startDate=19760801&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19761031&query=Jimmy%20Carter&sort=newest&startDate=19761001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19761231&query=Jimmy%20Carter&sort=newest&startDate=19761101&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_0, my_dat_1, my_dat_2, my_dat_3, my_dat_3_1, my_dat_4, my_dat_4_1], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1976_Jimmy_Carter.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d013249",
   "metadata": {},
   "source": [
    "# Election 1972 - Richard Nixon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47d81b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-219c35e958a1>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-219c35e958a1>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-219c35e958a1>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-219c35e958a1>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-219c35e958a1>:256: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-219c35e958a1>:307: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-219c35e958a1>:359: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-219c35e958a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m         \u001b[0mshow_more\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_to_be_clickable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'//button[@type=\"button\"][contains(.,\"Show More\")]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mshow_more\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1975\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19750331&query=Richard%20Nixon&sort=newest&startDate=19750101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19750630&query=Richard%20Nixon&sort=newest&startDate=19750401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19750930&query=Richard%20Nixon&sort=newest&startDate=19750701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19751231&query=Richard%20Nixon&sort=newest&startDate=19751001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1975_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1974\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19740331&query=Richard%20Nixon&sort=newest&startDate=19740101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19740630&query=Richard%20Nixon&sort=newest&startDate=19740401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19740731&query=Richard%20Nixon&sort=newest&startDate=19740701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19740930&query=Richard%20Nixon&sort=newest&startDate=19740801')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19741231&query=Richard%20Nixon&sort=newest&startDate=19741001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_3_1, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1974_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1973\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19730331&query=Richard%20Nixon&sort=newest&startDate=19730101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19730630&query=Richard%20Nixon&sort=newest&startDate=19730401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19730930&query=Richard%20Nixon&sort=newest&startDate=19730701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19731231&query=Richard%20Nixon&sort=newest&startDate=19731001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1973_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1972\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19720331&query=Richard%20Nixon&sort=newest&startDate=19720101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19720630&query=Richard%20Nixon&sort=newest&startDate=19720401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19720930&query=Richard%20Nixon&sort=newest&startDate=19720701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19721231&query=Richard%20Nixon&sort=newest&startDate=19721001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1972_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fe1da0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:53: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:102: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:184: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:235: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:287: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:336: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:418: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:469: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:521: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-8fad4f6dbcbc>:570: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19740731&query=Richard%20Nixon&sort=newest&startDate=19740701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19740930&query=Richard%20Nixon&sort=newest&startDate=19740801')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19741231&query=Richard%20Nixon&sort=newest&startDate=19741001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_3_1, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1974_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1973\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19730331&query=Richard%20Nixon&sort=newest&startDate=19730101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19730630&query=Richard%20Nixon&sort=newest&startDate=19730401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19730930&query=Richard%20Nixon&sort=newest&startDate=19730701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19731231&query=Richard%20Nixon&sort=newest&startDate=19731001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1973_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1972\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19720331&query=Richard%20Nixon&sort=newest&startDate=19720101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19720630&query=Richard%20Nixon&sort=newest&startDate=19720401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19720930&query=Richard%20Nixon&sort=newest&startDate=19720701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19721231&query=Richard%20Nixon&sort=newest&startDate=19721001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1972_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae88426",
   "metadata": {},
   "source": [
    "# Election 1968 - Richard Nixon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4549ce29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:256: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:307: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:359: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:408: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:490: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:541: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:593: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:642: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:724: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:775: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:827: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-98836a88a74a>:876: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-98836a88a74a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    881\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m         \u001b[0mshow_more\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_to_be_clickable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'//button[@type=\"button\"][contains(.,\"Show More\")]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m         \u001b[0mshow_more\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1971\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19710331&query=Richard%20Nixon&sort=newest&startDate=19710101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19710630&query=Richard%20Nixon&sort=newest&startDate=19710401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19710930&query=Richard%20Nixon&sort=newest&startDate=19710701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19711231&query=Richard%20Nixon&sort=newest&startDate=19711001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1971_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1970\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19700331&query=Richard%20Nixon&sort=newest&startDate=19700101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19700630&query=Richard%20Nixon&sort=newest&startDate=19700401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19700930&query=Richard%20Nixon&sort=newest&startDate=19700701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19701231&query=Richard%20Nixon&sort=newest&startDate=19701001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1970_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1969\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19690331&query=Richard%20Nixon&sort=newest&startDate=19690101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19690630&query=Richard%20Nixon&sort=newest&startDate=19690401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19690930&query=Richard%20Nixon&sort=newest&startDate=19690701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19691231&query=Richard%20Nixon&sort=newest&startDate=19691001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1969_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1968\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19680331&query=Richard%20Nixon&sort=newest&startDate=19680101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19680630&query=Richard%20Nixon&sort=newest&startDate=19680401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19680930&query=Richard%20Nixon&sort=newest&startDate=19680701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19681031&query=Richard%20Nixon&sort=newest&startDate=19681001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19681231&query=Richard%20Nixon&sort=newest&startDate=19681101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_5 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4, my_dat_5], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1968_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc8b2bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-d03083a91d7e>:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-d03083a91d7e>:53: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19681031&query=Richard%20Nixon&sort=newest&startDate=19681001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19681231&query=Richard%20Nixon&sort=newest&startDate=19681101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_5 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4, my_dat_5], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1968_Richard_Nixon.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51999896",
   "metadata": {},
   "source": [
    "# Election 1964 - Lyndon B. Johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5af60532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:73: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:127: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:179: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:228: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:311: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:362: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:414: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:463: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:545: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:596: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:648: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:697: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:779: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:830: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:882: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-8a60ed24f71e>:931: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1967\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19670228&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19670101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_0 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19670331&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19670301')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19670630&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19670401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19670930&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19670701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19671231&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19671001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_0, my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1967_Lyndon_Johnson.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1966\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19660331&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19660101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19660630&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19660401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19660930&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19660701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19661231&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19661001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1966_Lyndon_Johnson.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1965\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19650331&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19650101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19650630&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19650401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19650930&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19650701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19651231&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19651001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1965_Lyndon_Johnson.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1964\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19640331&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19640101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19640630&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19640401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19640930&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19640701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19641231&query=Lyndon%20B.%20Johnson&sort=newest&startDate=19641001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1964_Lyndon_Johnson.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bfde45",
   "metadata": {},
   "source": [
    "# Election 1960 - John F. Kennedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8225e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:72: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:124: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:173: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:256: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:307: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:359: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:408: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:490: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:541: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:593: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:642: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:724: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:775: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-c03be59691d9>:827: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c03be59691d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m         \u001b[0mshow_more\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_to_be_clickable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'//button[@type=\"button\"][contains(.,\"Show More\")]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m         \u001b[0mshow_more\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\wait.py\u001b[0m in \u001b[0;36muntil\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_driver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py\u001b[0m in \u001b[0;36m_predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWebElement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if given locator instead of WebElement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# grab element at locator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisibility_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m   1242\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'[name=\"%s\"]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0m\u001b[0;32m   1245\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m             'value': value})['value']\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    419\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{self._url}{path}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     76\u001b[0m             )\n\u001b[0;32m     77\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             return self.request_encode_body(\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1963\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19630331&query=John%20F.%20Kennedy&sort=newest&startDate=19630101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19630630&query=John%20F.%20Kennedy&sort=newest&startDate=19630401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19630930&query=John%20F.%20Kennedy&sort=newest&startDate=19630701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19631231&query=John%20F.%20Kennedy&sort=newest&startDate=19631001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1963_John_Kennedy.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1962\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19620331&query=John%20F.%20Kennedy&sort=newest&startDate=19620101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19620630&query=John%20F.%20Kennedy&sort=newest&startDate=19620401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19620930&query=John%20F.%20Kennedy&sort=newest&startDate=19620701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19621231&query=John%20F.%20Kennedy&sort=newest&startDate=19621001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1962_John_Kennedy.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1961\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19610331&query=John%20F.%20Kennedy&sort=newest&startDate=19610101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19610630&query=John%20F.%20Kennedy&sort=newest&startDate=19610401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19610930&query=John%20F.%20Kennedy&sort=newest&startDate=19610701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19611231&query=John%20F.%20Kennedy&sort=newest&startDate=19611001&types=article')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_3, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1961_John_Kennedy.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################---------------------------------- 1960\n",
    "\n",
    "\n",
    "# presidential elections\n",
    "\n",
    "# source: https://stackoverflow.com/questions/56250120/automation-using-selenium-and-web-scraping-using-beautifulsoup-in-python\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import string\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19600331&query=John%20F.%20Kennedy&sort=newest&startDate=19600101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19600531&query=John%20F.%20Kennedy&sort=newest&startDate=19600401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19600630&query=John%20F.%20Kennedy&sort=newest&startDate=19600601')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19600731&query=John%20F.%20Kennedy&sort=newest&startDate=19600701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19600930&query=John%20F.%20Kennedy&sort=newest&startDate=19600801')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19601130&query=John%20F.%20Kennedy&sort=newest&startDate=19601001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19601231&query=John%20F.%20Kennedy&sort=newest&startDate=19601201')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_2_1, my_dat_3, my_dat_3_1, my_dat_3_2, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1960_John_Kennedy.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55477ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-afb6e53fbb3e>:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-afb6e53fbb3e>:55: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-afb6e53fbb3e>:103: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-afb6e53fbb3e>:153: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-afb6e53fbb3e>:203: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-afb6e53fbb3e>:250: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-afb6e53fbb3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[0msearch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ol'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'data-testid'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'search-results'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mlink_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19600531&query=John%20F.%20Kennedy&sort=newest&startDate=19600401')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19600630&query=John%20F.%20Kennedy&sort=newest&startDate=19600601')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_2_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19600731&query=John%20F.%20Kennedy&sort=newest&startDate=19600701')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19600930&query=John%20F.%20Kennedy&sort=newest&startDate=19600801')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_1 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19601031&query=John%20F.%20Kennedy&sort=newest&startDate=19601001')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_2 = my_dat\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19601130&query=John%20F.%20Kennedy&sort=newest&startDate=19601101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19601231&query=John%20F.%20Kennedy&sort=newest&startDate=19601201')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_2_1, my_dat_3, my_dat_3_1, my_dat_3_2, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1960_John_Kennedy.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "795bcc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-c896bfd31a2b>:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-c896bfd31a2b>:51: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tOrdinal0 [0x00396903+2517251]\n",
      "\tOrdinal0 [0x0032F8E1+2095329]\n",
      "\tOrdinal0 [0x00232848+1058888]\n",
      "\tOrdinal0 [0x0025D448+1233992]\n",
      "\tOrdinal0 [0x0025D63B+1234491]\n",
      "\tOrdinal0 [0x00287812+1406994]\n",
      "\tOrdinal0 [0x0027650A+1336586]\n",
      "\tOrdinal0 [0x00285BBF+1399743]\n",
      "\tOrdinal0 [0x0027639B+1336219]\n",
      "\tOrdinal0 [0x002527A7+1189799]\n",
      "\tOrdinal0 [0x00253609+1193481]\n",
      "\tGetHandleVerifier [0x00525904+1577972]\n",
      "\tGetHandleVerifier [0x005D0B97+2279047]\n",
      "\tGetHandleVerifier [0x00426D09+534521]\n",
      "\tGetHandleVerifier [0x00425DB9+530601]\n",
      "\tOrdinal0 [0x00334FF9+2117625]\n",
      "\tOrdinal0 [0x003398A8+2136232]\n",
      "\tOrdinal0 [0x003399E2+2136546]\n",
      "\tOrdinal0 [0x00343541+2176321]\n",
      "\tBaseThreadInitThunk [0x7736FA29+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A9E+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77997A6E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19601130&query=John%20F.%20Kennedy&sort=newest&startDate=19601101')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_3_3 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "base = \"https://www.nytimes.com\"\n",
    "\n",
    "browser = webdriver.Chrome(r'C:\\Users\\scyth\\Downloads\\chromedriver.exe')\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "browser.get('https://www.nytimes.com/search?dropmab=false&endDate=19601231&query=John%20F.%20Kennedy&sort=newest&startDate=19601201')\n",
    "my_dat = [] # empty var\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        show_more = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@type=\"button\"][contains(.,\"Show More\")]')))  \n",
    "        show_more.click()\n",
    "    except Exception as e:\n",
    "            print(e)\n",
    "            break    \n",
    "\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')\n",
    "search_results = soup.find('ol', {'data-testid':'search-results'})\n",
    "\n",
    "links = search_results.find_all('a')\n",
    "for link in links:\n",
    "    link_url = link['href']\n",
    "    \n",
    "    node_1 = link.find_next('h4')\n",
    "    if node_1 is not None:\n",
    "        title = node_1.text\n",
    "    else:\n",
    "        title = None \n",
    "    \n",
    "    node = link.find_next('span')\n",
    "    if node is not None:\n",
    "        date = node.text\n",
    "    else:\n",
    "        date = None\n",
    "    my_dat.append((date, title))\n",
    "   # print(date + ': '+ title + '\\n')\n",
    "\n",
    "cols=['Subtitle','Title']\n",
    "my_dat = pd.DataFrame(my_dat, columns=cols)\n",
    "\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "# save as a dataset\n",
    "my_dat_4 = my_dat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########## merge all datasets\n",
    "my_dat = pd.concat([my_dat_1, my_dat_2, my_dat_2_1, my_dat_3, my_dat_3_1, my_dat_3_2, my_dat_4], ignore_index=True)\n",
    "# determining the name of the file\n",
    "file_name = '1960_John_Kennedy.xlsx'\n",
    "  \n",
    "# saving the excel\n",
    "my_dat.to_excel(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb8b8d59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_dat_3_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-ff8f0feb1cb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_dat_3_3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'my_dat_3_3' is not defined"
     ]
    }
   ],
   "source": [
    "my_dat_3_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc2061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
